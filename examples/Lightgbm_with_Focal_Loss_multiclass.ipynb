{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM with Focal Loss for Multiclass classification problems\n",
    "\n",
    "Let me show how to adapt the Focal Loss implementation for binary classification to a multiclass classification problem.\n",
    "\n",
    "The idea is to face the problem using the Binary Cross Entropy With Logits (borrowing from `Pytorch` notation `BCEWithLogitsLoss`). \n",
    "\n",
    "$$\n",
    "loss = -[y_{\\text true} \\cdot log\\sigma(x) + (1-y_{\\text true}) \\cdot log(1-\\sigma(x))] \n",
    "$$\n",
    "\n",
    "Where $\\sigma$ is the sigmoid function\n",
    "\n",
    "For example, let's assume we have a problem with 10 classes and we have two samples/observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.random.choice(11, (1,2))\n",
    "# from -2 to 2 to illustrate the fact the preds coming from lightGBM when using custom losses are NOT probs\n",
    "y_pred = np.random.uniform(low=-2, high=2, size=(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 8]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7815048 , -1.74097067, -1.43641606,  0.11109954,  1.31257876,\n",
       "         0.46640561,  1.01773531, -0.45675619,  1.71304683, -0.39435074],\n",
       "       [-1.90351294,  1.0885264 ,  0.16374925,  0.19066526,  0.35796125,\n",
       "        -0.19860415, -0.84501651, -1.65857055, -0.40721054,  0.36683568]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1./(1. +  np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / (np.sum(exp_x, axis=1, keepdims=True) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels one-hot encoded\n",
    "y_true_oh = np.eye(10)[y_true][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7512135455836538"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BCEWithLogitsLoss\n",
    "( -( y_true_oh * np.log(sigmoid(y_pred)) + (1-y_true_oh) * np.log(1-sigmoid(y_pred)) ) ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Focal Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb(y_pred, dtrain, alpha, gamma, num_class):\n",
    "    \"\"\"\n",
    "    Focal Loss for lightgbm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    num_class: int\n",
    "        number of classes\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    # N observations x num_class arrays\n",
    "    y_true = np.eye(num_class)[y_true.astype('int')]\n",
    "    y_pred = y_pred.reshape(-1,num_class)\n",
    "    # alpha and gamma multiplicative factors with BCEWithLogitsLoss\n",
    "    def fl(x,t):\n",
    "        p = 1/(1+np.exp(-x))\n",
    "        return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "    partial_fl = lambda x: fl(x, y_true)\n",
    "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "    # flatten in column-major (Fortran-style) order\n",
    "    return grad.flatten('F'), hess.flatten('F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it really. Now one would want/need the corresponding evalulation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb_eval_error(y_pred, dtrain, alpha, gamma, num_class):\n",
    "    \"\"\"\n",
    "    Focal Loss for lightgbm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    num_class: int\n",
    "        number of classes\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    y_true = np.eye(num_class)[y_true.astype('int')]\n",
    "    y_pred = y_pred.reshape(-1,num_class)\n",
    "    p = 1/(1+np.exp(-y_pred))\n",
    "    loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n",
    "    # a variant can be np.sum(loss)/num_class\n",
    "    return 'focal_loss', np.mean(loss), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from scipy.misc import derivative\n",
    "\n",
    "# very inadequate dataset as is perfectly balanced, but just to illustrate\n",
    "iris = datasets.load_iris()\n",
    "X_org = iris.data\n",
    "y_org = iris.target\n",
    "\n",
    "# shuffle...makes me feel good\n",
    "x = np.hstack([X_org,y_org.reshape(-1, 1)])\n",
    "np.random.shuffle(x)\n",
    "\n",
    "X = x[:, :4]\n",
    "y = x[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "lgbtrain = lgb.Dataset(X_tr, y_tr, free_raw_data=True)\n",
    "lgbeval = lgb.Dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's focal_loss: 0.101059\n",
      "[2]\tvalid_0's focal_loss: 0.101034\n",
      "[3]\tvalid_0's focal_loss: 0.101009\n",
      "[4]\tvalid_0's focal_loss: 0.100985\n",
      "[5]\tvalid_0's focal_loss: 0.10096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "focal_loss = lambda x,y: focal_loss_lgb(x, y, 0.25, 2., 3)\n",
    "eval_error = lambda x,y: focal_loss_lgb_eval_error(x, y, 0.25, 2., 3)\n",
    "params  = {'learning_rate':0.001, 'num_boost_round':5, 'num_class':3}\n",
    "# model = lgb.train(params, lgbtrain, fobj=focal_loss)\n",
    "model = lgb.train(params, lgbtrain, valid_sets=[lgbeval], fobj=focal_loss, feval=eval_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, np.argmax(softmax(model.predict(X_val)), axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
