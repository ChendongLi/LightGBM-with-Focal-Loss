{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to code and use the Focal Loss with LightGBM\n",
    "\n",
    "The [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf) for LightGBM can be coded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.misc import derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb(y_pred, dtrain, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Focal Loss for lightgbm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    def fl(x,t):\n",
    "        p = 1/(1+np.exp(-x))\n",
    "        return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "    partial_fl = lambda x: fl(x, y_true)\n",
    "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are going to use it as our custom loss, we also need our custom evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb_eval_error(y_pred, dtrain, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Adapation of the Focal Loss for lightgbm to be used as evaluation loss\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    p = 1/(1+np.exp(-y_pred))\n",
    "    loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n",
    "    return 'focal_loss', np.mean(loss), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use them, first we need to make them partial functions of **only** `y_pred` and `dtrain`, since this is a structural requirement for LighGBM. Then, we simply pass them as parameters. \n",
    "\n",
    "Let me first load some processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>education_occupation</th>\n",
       "      <th>native_country_occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11961</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>0.095890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16067</th>\n",
       "      <td>0.589041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12914</th>\n",
       "      <td>0.452055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479592</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6343</th>\n",
       "      <td>0.205479</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  workclass  education  marital_status  occupation  \\\n",
       "11961  0.287671          0          0               0           0   \n",
       "1230   0.095890          1          1               0           1   \n",
       "16067  0.589041          1          1               1           1   \n",
       "12914  0.452055          1          1               2           2   \n",
       "6343   0.205479          1          2               2           3   \n",
       "\n",
       "       relationship  race  gender  capital_gain  capital_loss  hours_per_week  \\\n",
       "11961             0     0       0           0.0           0.0        0.397959   \n",
       "1230              0     0       1           0.0           0.0        0.397959   \n",
       "16067             1     0       1           0.0           0.0        0.193878   \n",
       "12914             2     0       0           0.0           0.0        0.479592   \n",
       "6343              2     0       0           0.0           0.0        0.397959   \n",
       "\n",
       "       native_country  education_occupation  native_country_occupation  \n",
       "11961               0                     0                          0  \n",
       "1230                0                     1                          1  \n",
       "16067               0                     1                          1  \n",
       "12914               0                     2                          2  \n",
       "6343                0                     3                          3  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"../data/\")\n",
    "databunch = pickle.load(open(PATH/'adult_databunch.p', 'rb'))\n",
    "colnames = databunch.colnames\n",
    "categorical_columns = databunch.categorical_columns + databunch.crossed_columns\n",
    "X = databunch.data\n",
    "y = databunch.target\n",
    "# you know, in real life, train, valid AND test, and you keep it somewhere safe...\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.25,\n",
    "    random_state=1, stratify=y)\n",
    "# let's have a look:\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM with Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgtrain = lgb.Dataset(\n",
    "    X_tr, y_tr,\n",
    "    feature_name=colnames,\n",
    "    categorical_feature = categorical_columns,\n",
    "    free_raw_data=False)\n",
    "lgvalid = lgtrain.create_valid(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.6/site-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.6/site-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['education', 'education_occupation', 'gender', 'marital_status', 'native_country', 'native_country_occupation', 'occupation', 'race', 'relationship', 'workclass']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "/usr/local/lib/python3.6/site-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's focal_loss: 0.098781\n",
      "[2]\tvalid_0's focal_loss: 0.0898273\n",
      "[3]\tvalid_0's focal_loss: 0.0821333\n",
      "[4]\tvalid_0's focal_loss: 0.0755058\n",
      "[5]\tvalid_0's focal_loss: 0.0697994\n",
      "[6]\tvalid_0's focal_loss: 0.064839\n",
      "[7]\tvalid_0's focal_loss: 0.0605124\n",
      "[8]\tvalid_0's focal_loss: 0.0567805\n",
      "[9]\tvalid_0's focal_loss: 0.0534902\n",
      "[10]\tvalid_0's focal_loss: 0.0506304\n"
     ]
    }
   ],
   "source": [
    "focal_loss = lambda x,y: focal_loss_lgb(x, y, 0.25, 2.)\n",
    "eval_error = lambda x,y: focal_loss_lgb_eval_error(x, y, 0.25, 2.)\n",
    "params  = {'learning_rate':0.1, 'num_boost_round':10}\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgtrain,\n",
    "    valid_sets=[lgvalid],\n",
    "    fobj=focal_loss,\n",
    "    feval=eval_error\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn's API\n",
    "\n",
    "If you prefer to use LightGBM's sklearn API, simply replace `dtrain` with `y_true`, and swap the predictions and ground truth order, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb_sk(y_true, y_pred, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Focal Loss for lightgbm\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    def fl(x,t):\n",
    "        p = 1/(1+np.exp(-x))\n",
    "        return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "    partial_fl = lambda x: fl(x, y_true)\n",
    "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_lgb_eval_error_sk(y_true, y_pred, alpha, gamma):\n",
    "    \"\"\"\n",
    "    Adapation of the Focal Loss for lightgbm to be used as evaluation loss\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_pred: numpy.ndarray\n",
    "        array with the predictions\n",
    "    dtrain: lightgbm.Dataset\n",
    "    alpha, gamma: float\n",
    "        See original paper https://arxiv.org/pdf/1708.02002.pdf\n",
    "    \"\"\"\n",
    "    a,g = alpha, gamma\n",
    "    p = 1/(1+np.exp(-y_pred))\n",
    "    loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n",
    "    return 'focal_loss', np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's focal_loss: 0.0988352\n",
      "[2]\tvalid_0's focal_loss: 0.0899494\n",
      "[3]\tvalid_0's focal_loss: 0.0823239\n",
      "[4]\tvalid_0's focal_loss: 0.0757314\n",
      "[5]\tvalid_0's focal_loss: 0.0700502\n",
      "[6]\tvalid_0's focal_loss: 0.0651475\n",
      "[7]\tvalid_0's focal_loss: 0.0608702\n",
      "[8]\tvalid_0's focal_loss: 0.0571672\n",
      "[9]\tvalid_0's focal_loss: 0.0539455\n",
      "[10]\tvalid_0's focal_loss: 0.051152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_boost_round=10, num_leaves=31,\n",
       "               objective=<function <lambda> at 0x115bc1950>, random_state=None,\n",
       "               reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss = lambda x,y: focal_loss_lgb_sk(x, y, 0.25, 2.)\n",
    "eval_error = lambda x,y: focal_loss_lgb_eval_error_sk(x, y, 0.25, 2.)\n",
    "model = lgb.LGBMClassifier(objective=focal_loss, learning_rate=0.1, num_boost_round=10)\n",
    "model.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=eval_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
